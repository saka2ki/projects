{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "314d7728-5d09-48ae-8bc7-c96246871703",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "from flax import linen as nn\n",
    "from jax.nn.initializers import lecun_normal, normal\n",
    "from jax.numpy.linalg import eigh, inv, matrix_power\n",
    "from jax.scipy.signal import convolve\n",
    "\n",
    "from ssm import *\n",
    "rng = jax.random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8468c2a5-74e1-4185-b47a-65de52151470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_step_initializer(dt_min=0.001, dt_max=0.1):\n",
    "    def init(key, shape):\n",
    "        return jax.random.uniform(key, shape) * (\n",
    "            np.log(dt_max) - np.log(dt_min)\n",
    "        ) + np.log(dt_min)\n",
    "\n",
    "    return init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ab8e048-30e5-43b4-925e-f178c531d1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSMLayer(nn.Module):\n",
    "    N: int\n",
    "    l_max: int\n",
    "    decode: bool = False\n",
    "\n",
    "    def setup(self):\n",
    "        # SSM parameters\n",
    "        self.A = self.param(\"A\", lecun_normal(), (self.N, self.N))\n",
    "        self.B = self.param(\"B\", lecun_normal(), (self.N, 1))\n",
    "        self.C = self.param(\"C\", lecun_normal(), (1, self.N))\n",
    "        self.D = self.param(\"D\", nn.initializers.ones, (1,))\n",
    "\n",
    "        # Step parameter\n",
    "        self.log_step = self.param(\"log_step\", log_step_initializer(), (1,))\n",
    "\n",
    "        step = np.exp(self.log_step)\n",
    "        self.ssm = discretize(self.A, self.B, self.C, step=step)\n",
    "        self.K = K_conv(*self.ssm, self.l_max)\n",
    "\n",
    "        # RNN cache for long sequences\n",
    "        self.x_k_1 = self.variable(\"cache\", \"cache_x_k\", np.zeros, (self.N,))\n",
    "\n",
    "    def __call__(self, u):\n",
    "        if not self.decode:\n",
    "            # CNN Mode\n",
    "            return causal_convolution(u, self.K) + self.D * u\n",
    "        else:\n",
    "            # RNN Mode\n",
    "            x_k, y_s = scan_SSM(*self.ssm, u[:, np.newaxis], self.x_k_1.value)\n",
    "            if self.is_mutable_collection(\"cache\"):\n",
    "                self.x_k_1.value = x_k\n",
    "            return y_s.reshape(-1).real + self.D * u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f46a9d64-fb60-42e6-99e9-893473e56a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cloneLayer(layer):\n",
    "    return nn.vmap(\n",
    "        layer,\n",
    "        in_axes=1,\n",
    "        out_axes=1,\n",
    "        variable_axes={\"params\": 1, \"cache\": 1, \"prime\": 1},\n",
    "        split_rngs={\"params\": True},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f6c2674-da89-4d12-a60a-83d6f821742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSMLayer = cloneLayer(SSMLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19644d3-c5c2-40ae-9936-f4f984a06db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceBlock(nn.Module):\n",
    "    layer_cls: nn.Module\n",
    "    layer: dict  # Hyperparameters of inner layer\n",
    "    dropout: float\n",
    "    d_model: int\n",
    "    prenorm: bool = True\n",
    "    glu: bool = True\n",
    "    training: bool = True\n",
    "    decode: bool = False\n",
    "\n",
    "    def setup(self):\n",
    "        self.seq = self.layer_cls(**self.layer, decode=self.decode)\n",
    "        self.norm = nn.LayerNorm()\n",
    "        self.out = nn.Dense(self.d_model)\n",
    "        if self.glu:\n",
    "            self.out2 = nn.Dense(self.d_model)\n",
    "        self.drop = nn.Dropout(\n",
    "            self.dropout,\n",
    "            broadcast_dims=[0],\n",
    "            deterministic=not self.training,\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        skip = x\n",
    "        if self.prenorm:\n",
    "            x = self.norm(x)\n",
    "        x = self.seq(x)\n",
    "        x = self.drop(nn.gelu(x))\n",
    "        if self.glu:\n",
    "            x = self.out(x) * jax.nn.sigmoid(self.out2(x))\n",
    "        else:\n",
    "            x = self.out(x)\n",
    "        x = skip + self.drop(x)\n",
    "        if not self.prenorm:\n",
    "            x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe5dd4-beb3-4100-9c6a-8ad365b8ec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Embed):\n",
    "    num_embeddings: int\n",
    "    features: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        y = nn.Embed(self.num_embeddings, self.features)(x[..., 0])\n",
    "        return np.where(x > 0, y, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e751ad-fe06-40f5-931f-89242d93c3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedModel(nn.Module):\n",
    "    layer_cls: nn.Module\n",
    "    layer: dict  # Extra arguments to pass into layer constructor\n",
    "    d_output: int\n",
    "    d_model: int\n",
    "    n_layers: int\n",
    "    prenorm: bool = True\n",
    "    dropout: float = 0.0\n",
    "    embedding: bool = False  # Use nn.Embed instead of nn.Dense encoder\n",
    "    classification: bool = False\n",
    "    training: bool = True\n",
    "    decode: bool = False  # Probably should be moved into layer_args\n",
    "\n",
    "    def setup(self):\n",
    "        if self.embedding:\n",
    "            self.encoder = Embedding(self.d_output, self.d_model)\n",
    "        else:\n",
    "            self.encoder = nn.Dense(self.d_model)\n",
    "        self.decoder = nn.Dense(self.d_output)\n",
    "        self.layers = [\n",
    "            SequenceBlock(\n",
    "                layer_cls=self.layer_cls,\n",
    "                layer=self.layer,\n",
    "                prenorm=self.prenorm,\n",
    "                d_model=self.d_model,\n",
    "                dropout=self.dropout,\n",
    "                training=self.training,\n",
    "                decode=self.decode,\n",
    "            )\n",
    "            for _ in range(self.n_layers)\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if not self.classification:\n",
    "            if not self.embedding:\n",
    "                x = x / 255.0  # Normalize\n",
    "            if not self.decode:\n",
    "                x = np.pad(x[:-1], [(1, 0), (0, 0)])\n",
    "        x = self.encoder(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        if self.classification:\n",
    "            x = np.mean(x, axis=0)\n",
    "        x = self.decoder(x)\n",
    "        return nn.log_softmax(x, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a7958-1a15-4d43-95f2-87446ae0ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "BatchStackedModel = nn.vmap(\n",
    "    StackedModel,\n",
    "    in_axes=0,\n",
    "    out_axes=0,\n",
    "    variable_axes={\"params\": None, \"dropout\": None, \"cache\": 0, \"prime\": None},\n",
    "    split_rngs={\"params\": False, \"dropout\": True},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
