{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6891f277",
   "metadata": {},
   "source": [
    "最大シーケンス長に分割されたJOSONLに整形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90141438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import path as path\n",
    "\n",
    "output_file = path.wiki_batched\n",
    "with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "    stack = \"\"\n",
    "    with open(path.wiki, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            if len(data[\"text\"]) <= 10: continue\n",
    "            elif len(data[\"text\"]) <= path.max_seq_len // 2:\n",
    "                stack += data[\"text\"] + \"<pad>\"\n",
    "                if len(stack) > path.max_seq_len // 2:\n",
    "                    json.dump({\"text\": stack}, json_file, ensure_ascii=False)\n",
    "                    json_file.write('\\n')\n",
    "                    stack = \"\"\n",
    "            elif len(data[\"text\"]) > path.max_seq_len:\n",
    "                split_index = [0]\n",
    "                split_num = len(data[\"text\"]) // path.max_seq_len + 1\n",
    "                split_size = len(data[\"text\"]) // split_num\n",
    "                for i in range(split_num-1):\n",
    "                    split_index.append(data[\"text\"].rfind(\"。\", split_index[i], split_index[i] + split_size)+1)\n",
    "                    json.dump({\"text\": data[\"text\"][split_index[i]:split_index[i+1]]}, json_file, ensure_ascii=False)\n",
    "                    json_file.write('\\n')\n",
    "                json.dump({\"text\": data[\"text\"][split_index[-1]:]}, json_file, ensure_ascii=False)\n",
    "                json_file.write('\\n')\n",
    "            else:\n",
    "                json.dump({\"text\": data[\"text\"]}, json_file, ensure_ascii=False)\n",
    "                json_file.write('\\n')\n",
    "            #break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e516000e",
   "metadata": {},
   "source": [
    "データセットの作成しPT形式で保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6f6a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import path as path\n",
    "from charTokenizer import CharTokenizer\n",
    "from dataset import JPNDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = CharTokenizer()\n",
    "with open(path.charTokenizer, 'r', encoding='utf-8') as f:\n",
    "    tokenizer.vocab = json.load(f)\n",
    "\n",
    "dataset = JPNDataset(path.wiki_batched, tokenizer, max_seq_len=path.max_seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "batch = next(iter(dataloader))\n",
    "print(batch)\n",
    "\n",
    "torch.save(dataset, f\"../data/dataset_{path.max_seq_len}.pt\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
