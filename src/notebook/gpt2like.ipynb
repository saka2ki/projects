{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T14:19:34.762839Z",
     "start_time": "2025-05-11T14:19:33.357254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, max_seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, max_seq_len, d_model))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'ln1': nn.LayerNorm(d_model),\n",
    "                'attn': nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True),\n",
    "                'ln2': nn.LayerNorm(d_model),\n",
    "                'mlp': nn.Sequential(\n",
    "                    nn.Linear(d_model, 4 * d_model),\n",
    "                    nn.GELU(),\n",
    "                    nn.Linear(4 * d_model, d_model),\n",
    "                    nn.Dropout(dropout),\n",
    "                )\n",
    "            }) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "        if isinstance(module, nn.LayerNorm) and module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()\n",
    "        x = self.token_emb(x) + self.pos_emb[:,:T]\n",
    "        x = self.dropout(x)\n",
    "        attn_mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x_norm = layer['ln1'](x)\n",
    "            attn_output, _ = layer['attn'](\n",
    "                x_norm, x_norm, x_norm,\n",
    "                attn_mask=attn_mask,\n",
    "                need_weights=False,\n",
    "            )\n",
    "            x = x + attn_output\n",
    "            x = x + layer['mlp'](layer['ln2'](x))\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        return logits"
   ],
   "id": "48d09928398bc27c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T14:19:35.476375Z",
     "start_time": "2025-05-11T14:19:34.890838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from bitsandbytes import optim as bnb_optim\n",
    "\n",
    "def train(model, dataloader, vocab_size, epochs=3, lr=3e-4, weight_decay=0.0, early_stop_loss=0.1):\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "\n",
    "    model.to(device)\n",
    "    optimizer = bnb_optim.AdamW8bit(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    model, optimizer, dataloader = accelerator.prepare(model, optimizer, dataloader)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch: {epoch+1}\", disable=not accelerator.is_local_main_process)\n",
    "        for batch in pbar:\n",
    "            input_ids = batch[:, :-1]\n",
    "            labels = batch[:, 1:]\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            #loss = criterion(outputs.view(-1, vocab_size), labels.view(-1))\n",
    "            loss = criterion(outputs.reshape(-1, vocab_size), labels.reshape(-1))\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "            if loss.item() < early_stop_loss:\n",
    "                print(f\"Training stopped early at epoch {epoch+1}, batch {pbar.n} due to train_loss < {early_stop_loss}\")\n",
    "                return  # 訓練を中止"
   ],
   "id": "aca2607a562e796d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-11T14:19:35.483377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from src.notebook.dataloader import JPNDataset\n",
    "\n",
    "dataset = torch.load(\"../data/dataset_1024.pt\", weights_only=False)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "model = Decoder(vocab_size=22217, d_model=768, n_heads=12, n_layers=12, max_seq_len=1024, dropout=0.1)\n",
    "train(model, dataloader, vocab_size=22217, epochs=3, lr=3e-4, weight_decay=0.01)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1:   0%|          | 37/148034 [23:25<1772:26:15, 43.11s/it, loss=6]   "
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T14:18:24.231914200Z",
     "start_time": "2025-05-11T05:45:50.030611Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), \"../data/model.pth\")",
   "id": "b366ed067c2b23a0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T14:18:24.232915Z",
     "start_time": "2025-05-11T14:08:24.532281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import json\n",
    "import src.notebook.path as path\n",
    "from src.notebook.charTokenizer import CharTokenizer\n",
    "\n",
    "model = Decoder(vocab_size=22217, d_model=768, n_heads=12, n_layers=12, max_seq_len=1024)\n",
    "model.load_state_dict(torch.load(\"../data/model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "tokenizer = CharTokenizer()\n",
    "with open(path.charTokenizer, 'r', encoding='utf-8') as f:\n",
    "    tokenizer.vocab = json.load(f)\n",
    "inputs = tokenizer(\n",
    "    \"荒れた内を避ける為か中間付近までは馬場の中央付近を走行。道中ペースを緩め脚をためると、そのまま直線も先頭でゴールした。逃げての上がりは32.9で上がり最速タイ。 武豊騎手は「ポンと出て、無理に引っ張ることもなく、マイペースで行けた。ラストでひと伸びして能力の高さを感じた」とコメント。ききょうステークス以来実に1年ぶりの勝利を飾り、素質の高さを見せた。続く逆瀬川ステークスでは前走の走りを評価されてか、古馬と同じ55kgの斤量を課された。チャンピオンズカップの裏開催であった為、武豊騎手から吉田隼人騎手に乗り替わり、朝日杯FS以来のコンビ結成となった。スムーズにゲートを出ると、そのまま内3番手を追走。最後直線は力強く抜け出して、2連勝でのOP入りを決めた。吉田隼人騎手は「約1年ぶりに乗せていただきましたが成長しています。出たなりでいい位置をキープできました。抜け出してから、左にもたれる癖はあるが、上がり勝負にも対応してくれました。競馬に幅が広がったし、これからが楽しみです」と振り返った。2023年（4歳）.明け4歳の始動戦に選ばれたのは東京芝2000mのリステッド戦である[白富士ステークスとされた。当日は前走ローズステークス2着と好走したサリエラに次ぐ2番人気に評価された。レースはドーブネが最内枠から好スタートでハナを奪い、武豊のエスコートで1000m59.9秒という絶妙な時計で逃げを打つ。直線に入り粘りの逃げで後続を離すかに思えたが、残り200mあたりから失速。最後は後方から末脚を伸ばしてきたサリエラに交わされた。それでも内を通って伸びてきていたヤマニンサルバムには抜かせず2着を確保した。レース後武豊は「楽なペースだったけどね…。1ハロンぐらい少し距離が長いのかな」とコメントを残し、2000ｍはドーブネにとって距離が長い可能性が示唆された。\"\n",
    "    , max_length=1024, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs[\"input_ids\"])\n",
    "    predictions = torch.argmax(outputs, dim=-1)\n",
    "output_text = tokenizer.decode(predictions[0])\n",
    "print(output_text)\n"
   ],
   "id": "45b6fafc308ffe04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "荒れた内をたける為か中間付近までは馬場の中央付近を走行。道中ペースをけめ脚をためると、そのまま直線も先頭でゴールした。逃げての上がりは32.9で上がり最速タイ。 武豊騎手は「ポンと出て、無理に引っ張ることもなく、マイペースで行けた。ラストでひと伸びして能力の高さを感じた」とコメント。ききょうステークス以来実に1年ぶりの勝利を飾り、素質の高さを見せた。続く逆瀬川ステークスでは前走の走りを評価されてか、古馬と同じ55kgの滑量を課された。チャンピオンズカップの裏開催であった為、武豊騎手から吉田+人騎手に乗り替わり、朝日杯FS以来のコンビ結成となった。スムーズにゲートを出ると、そのまま内3番手を追走。最後直線は力強く抜け出して、2連勝でのOP入りを決めた。吉田+人騎手は「約1年ぶりに乗せていただきましたが成長しています。出たなりでいい位置をキープできました。抜け出してから、左にもたれる布はあるが、上がり勝負にも対応してくれました。競馬に幅が広がったし、これからが楽しみです」と振り返った。2023年（4歳）.明け4歳の始動戦に選ばれたのは東京芝2000mのリステッド戦である[白富士ステークスとされた。当日は前走ローズステークス2着と好走したサリエラに次ぐ2番人気に評価された。レースはドーブネが最内枠から好スタートでハナを奪い、武豊のエスコートで1000m59.9秒という絶準な時計で逃げを打つ。直線に入り括りの逃げで後続を離すかに思えたが、残り200mあたりから失速。最後は後方から末脚を伸ばしてきたサリエラに交わされた。それでも内を通って伸びてきていたヤマニンサルバムには抜かせず2着を確保した。レース後武豊は「楽なペースだったけどね…。1ハロンぐらい少し距離が長いのかな」とコメントを残し、2000『はドーブネにとって距離が長い可能性が示唆された。顔顔顔替顔顔顔顔顔顔誉顔顔緩顔盤誉盤顔誉顔概顔替誉概概顔概盤概替概概概盤概誉顔概概概概概概概概概概概概概誉顔概概誉概概概概概概概概概概概概盤概概概概概概概概概概概盤概概概概概誉概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概赤概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概概赤概概概概概概概概概概概概赤赤概概概系概概概概概概概概概系概概概概)概概概概)概赤系系概概概概概赤概赤赤概赤概赤赤概概赤概赤赤概エ概赤\n"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
