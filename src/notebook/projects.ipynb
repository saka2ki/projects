{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85277f42",
   "metadata": {},
   "source": [
    "バッチサイズに適したJSONLに整形"
   ]
  },
  {
   "cell_type": "code",
   "id": "9651e514",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T08:11:25.955374Z",
     "start_time": "2025-05-14T08:10:03.067017Z"
    }
   },
   "source": [
    "import json\n",
    "import path as path\n",
    "\n",
    "output_file = path.wiki_batched\n",
    "with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "    stack = \"\"\n",
    "    with open(path.wiki, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            if len(data[\"text\"]) <= 10: continue\n",
    "            elif len(data[\"text\"]) <= path.max_seq_len // 2:\n",
    "                stack += data[\"text\"] + \"<pad>\"\n",
    "                if len(stack) > 512:\n",
    "                    json.dump({\"text\": stack}, json_file, ensure_ascii=False)\n",
    "                    json_file.write('\\n')\n",
    "                    stack = \"\"\n",
    "            elif len(data[\"text\"]) > path.max_seq_len:\n",
    "                split_index = [0]\n",
    "                split_num = len(data[\"text\"]) // path.max_seq_len + 1\n",
    "                split_size = len(data[\"text\"]) // split_num\n",
    "                for i in range(split_num-1):\n",
    "                    split_index.append(data[\"text\"].rfind(\"。\", split_index[i], split_index[i] + split_size)+1)\n",
    "                    json.dump({\"text\": data[\"text\"][split_index[i]:split_index[i+1]]}, json_file, ensure_ascii=False)\n",
    "                    json_file.write('\\n')\n",
    "                json.dump({\"text\": data[\"text\"][split_index[-1]:]}, json_file, ensure_ascii=False)\n",
    "                json_file.write('\\n')\n",
    "            else:\n",
    "                json.dump({\"text\": data[\"text\"]}, json_file, ensure_ascii=False)\n",
    "                json_file.write('\\n')\n",
    "            #break"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "1df05281",
   "metadata": {},
   "source": [
    "データセットを作成し、PT形式で保存する"
   ]
  },
  {
   "cell_type": "code",
   "id": "38c2d426",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T08:14:28.524706Z",
     "start_time": "2025-05-14T08:11:25.984511Z"
    }
   },
   "source": [
    "import json\n",
    "import path as path\n",
    "from charTokenizer import CharTokenizer\n",
    "from dataset import JPNDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = CharTokenizer()\n",
    "with open(path.charTokenizer, 'r', encoding='utf-8') as f:\n",
    "    tokenizer.vocab = json.load(f)\n",
    "\n",
    "dataset = JPNDataset(path.wiki_batched, tokenizer, max_seq_len=path.max_seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "batch = next(iter(dataloader))\n",
    "print(batch)\n",
    "\n",
    "torch.save(dataset, f\"../data/dataset_{path.max_seq_len}.pt\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 597, 1728,  484,  ...,    0,    0,    0],\n",
      "        [2869,   20,  235,  ...,   52,  189,   97],\n",
      "        [ 132,  330,   38,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 410,  179,   13,  ...,    0,    0,    0],\n",
      "        [ 781,  262, 1867,  ...,    0,    0,    0],\n",
      "        [ 543,  731,  543,  ...,    0,    0,    0]])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "be9c5fa4",
   "metadata": {},
   "source": [
    "モデルを学習させ、PTH形式で保存する"
   ]
  },
  {
   "cell_type": "code",
   "id": "490fe220",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T08:16:48.040276Z",
     "start_time": "2025-05-14T08:16:47.385535Z"
    }
   },
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from bitsandbytes import optim as bnb_optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def train(model, dataloader, vocab_size, epochs=3, lr=3e-4, weight_decay=0.0, early_stop_loss=0.1):\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "\n",
    "    model.to(device)\n",
    "    optimizer = bnb_optim.AdamW8bit(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    model, optimizer, dataloader = accelerator.prepare(model, optimizer, dataloader)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch: {epoch+1}\", disable=not accelerator.is_local_main_process)\n",
    "        for batch in pbar:\n",
    "            input_ids = batch[:, :-1]\n",
    "            labels = batch[:, 1:]\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "\n",
    "            loss = criterion(outputs.reshape(-1, vocab_size), labels.reshape(-1))\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "            if loss.item() < early_stop_loss:\n",
    "                print(f\"Training stopped early at epoch {epoch+1}, batch {pbar.n} due to train_loss < {early_stop_loss}\")\n",
    "                return  # 訓練を中止"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "e569398c",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-14T08:17:28.688569Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import path as path\n",
    "from dataset import JPNDataset\n",
    "from model import Decoder\n",
    "\n",
    "dataset = torch.load(f\"../data/dataset_{path.max_seq_len}.pt\", weights_only=False)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = Decoder(vocab_size=22217, d_model=512, n_heads=1, n_layers=1, max_seq_len=path.max_seq_len, dropout=0.1)\n",
    "train(model, dataloader, vocab_size=22217, epochs=3, lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "torch.save(model.state_dict(), f\"../data/model_{path.max_seq_len}.pth\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1:   0%|          | 45/100312 [03:38<131:53:09,  4.74s/it, loss=5.29]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "76d283b4",
   "metadata": {},
   "source": [
    "推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85e8e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import path as path\n",
    "from charTokenizer import CharTokenizer\n",
    "\n",
    "model = Decoder(vocab_size=22217, d_model=512, n_heads=1, n_layers=1, max_seq_len=path.max_seq_len)\n",
    "model.load_state_dict(torch.load(f\"../data/model_{path.max_seq_len}.pth\"))\n",
    "model.eval()\n",
    "\n",
    "tokenizer = CharTokenizer()\n",
    "with open(path.charTokenizer, 'r', encoding='utf-8') as f:\n",
    "    tokenizer.vocab = json.load(f)\n",
    "inputs = tokenizer(\n",
    "    \"荒れた内を避ける為か中間付近までは馬場の中央付近を走行。道中ペースを緩め脚をためると、そのまま直線も先頭でゴールした。逃げての上がりは32.9で上がり最速タイ。 武豊騎手は「ポンと出て、無理に引っ張ることもなく、マイペースで行けた。ラストでひと伸びして能力の高さを感じた」とコメント。ききょうステークス以来実に1年ぶりの勝利を飾り、素質の高さを見せた。続く逆瀬川ステークスでは前走の走りを評価されてか、古馬と同じ55kgの斤量を課された。チャンピオンズカップの裏開催であった為、武豊騎手から吉田隼人騎手に乗り替わり、朝日杯FS以来のコンビ結成となった。スムーズにゲートを出ると、そのまま内3番手を追走。最後直線は力強く抜け出して、2連勝でのOP入りを決めた。吉田隼人騎手は「約1年ぶりに乗せていただきましたが成長しています。出たなりでいい位置をキープできました。抜け出してから、左にもたれる癖はあるが、上がり勝負にも対応してくれました。競馬に幅が広がったし、これからが楽しみです」と振り返った。2023年（4歳）.明け4歳の始動戦に選ばれたのは東京芝2000mのリステッド戦である[白富士ステークスとされた。当日は前走ローズステークス2着と好走したサリエラに次ぐ2番人気に評価された。レースはドーブネが最内枠から好スタートでハナを奪い、武豊のエスコートで1000m59.9秒という絶妙な時計で逃げを打つ。直線に入り粘りの逃げで後続を離すかに思えたが、残り200mあたりから失速。最後は後方から末脚を伸ばしてきたサリエラに交わされた。それでも内を通って伸びてきていたヤマニンサルバムには抜かせず2着を確保した。レース後武豊は「楽なペースだったけどね…。1ハロンぐらい少し距離が長いのかな」とコメントを残し、2000ｍはドーブネにとって距離が長い可能性が示唆された。\"\n",
    "    , max_seq_len=path.max_seq_len, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs[\"input_ids\"])\n",
    "    predictions = torch.argmax(outputs, dim=-1)\n",
    "output_text = tokenizer.decode(predictions[0])\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
