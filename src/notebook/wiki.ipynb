{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ab0513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import path as path\n",
    "output_file = path.wiki\n",
    "root_dir = '../data/text'\n",
    "\n",
    "# メモリ効率よくファイルを逐次的に処理\n",
    "with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "    #json_file.write('[')  # JSON配列の開始\n",
    "    first_entry = True\n",
    "\n",
    "    # root_dir内のすべてのサブディレクトリとファイルを探す\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            #json_file.write('[')\n",
    "            #first_entry = True\n",
    "\n",
    "            # ファイルを開いて、内容を処理\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = re.sub(r'\\b(\\w+)\\b(?:\\n{2,})\\1\\b', r'\\1', f.read())\n",
    "\n",
    "                content = re.sub(r'\\n.*?\\.', r'', content, re.MULTILINE)\n",
    "\n",
    "                content = re.sub(r'\\n', '', content)\n",
    "\n",
    "                # docタグごとにJSONに変換\n",
    "                docs = re.findall(r'<doc.*?>(.*?)</doc>', content, re.DOTALL)\n",
    "                for doc in docs:\n",
    "                    if not first_entry:\n",
    "                        json_file.write('\\n')  # 最初のエントリでない場合はカンマを付ける\n",
    "                    json.dump({\"text\": doc.strip()}, json_file, ensure_ascii=False)\n",
    "                    first_entry = False\n",
    "            #json_file.write(']')\n",
    "            #break\n",
    "    #json_file.write(']')  # JSON配列の終了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77baf54",
   "metadata": {},
   "source": [
    "トークナイザの語彙生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22db5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import path as path\n",
    "from charTokenizer import CharTokenizer\n",
    "\n",
    "tokenizer = CharTokenizer()\n",
    "with open(path.wiki, 'r', encoding='utf-8') as f:\n",
    "    for text in f:\n",
    "        tokenizer.build_vocab(text)\n",
    "\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be957f3b",
   "metadata": {},
   "source": [
    "トークナイザの語彙をJSON形式で保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4811ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(path.charTokenizer, 'w', encoding='utf-8') as f:\n",
    "    json.dump(tokenizer.vocab, f, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
